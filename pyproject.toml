[project]
name = "nanovllm"
version = "0.2.0"
authors = [{ name = "Xingkai Yu" }]
license = "MIT"
license-files = ["LICENSE"]
readme = "README.md"
description = "a lightweight vLLM implementation built from scratch"
requires-python = "==3.12.*"
dependencies = [
    "torch==2.8.0+cu129",
    "triton",
    "transformers>=4.51.3",
    "flash-attn",
    "xxhash",
    "batch-invariant-ops",
    "flash_attn_3"
]

[build-system]
requires = ["uv_build>=0.8.19,<0.9.0"]
build-backend = "uv_build"

[tool.uv.sources]
torch = [
  { index = "pytorch-cu129" },
]
torchvision = [
  { index = "pytorch-cu129" },
]
batch-invariant-ops = { git = "https://github.com/thinking-machines-lab/batch_invariant_ops" }
# 3rd party prebuilt FA3 wheel: https://windreamer.github.io/flash-attention3-wheels/
flash_attn_3 = { url = "https://github.com/windreamer/flash-attention3-wheels/releases/download/2025.09.28/flash_attn_3-3.0.0b1%2B20250928.cu129torch280cxx11abitrue.5059fd-cp39-abi3-linux_x86_64.whl" }

[[tool.uv.index]]
name = "pytorch-cu129"
url = "https://download.pytorch.org/whl/cu129"
explicit = true

[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true }]

[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE" }
